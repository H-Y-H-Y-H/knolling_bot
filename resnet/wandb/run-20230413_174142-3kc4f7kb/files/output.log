Device: cuda:0
this is num of close 1000
this is num of normal 1000
begin!
Training_Loss At Epoch 0:	0.06704134388826788
Testing_Loss At Epoch 0:	0.05665016256272793
0 time used:  109.35858750343323 lr: [0.0001]
Training_Loss At Epoch 1:	0.052862102957442404
Testing_Loss At Epoch 1:	0.04701740706339479
1 time used:  105.80904936790466 lr: [0.0001]
2 time used:  105.66328358650208 lr: [0.0001]
3 time used:  105.87943935394287 lr: [0.0001]
4 time used:  105.688725233078 lr: [0.0001]
Training_Loss At Epoch 5:	0.041933056758716704
Testing_Loss At Epoch 5:	0.04217178238555789
5 time used:  106.1235761642456 lr: [0.0001]
Training_Loss At Epoch 6:	0.040378504609689114
Testing_Loss At Epoch 6:	0.04208794040605426
6 time used:  106.23821234703064 lr: [0.0001]
Training_Loss At Epoch 7:	0.042988870688714084
Testing_Loss At Epoch 7:	0.04136660099029541
7 time used:  106.2794337272644 lr: [0.0001]
Training_Loss At Epoch 8:	0.039682684815488756
Testing_Loss At Epoch 8:	0.04037284504622221
8 time used:  106.28400492668152 lr: [0.0001]
9 time used:  105.83740878105164 lr: [0.0001]
10 time used:  105.7729480266571 lr: [0.0001]
Training_Loss At Epoch 11:	0.039623562963679436
Testing_Loss At Epoch 11:	0.039004558604210614
11 time used:  106.21270298957825 lr: [1e-05]
Training_Loss At Epoch 12:	0.03542619625339285
Testing_Loss At Epoch 12:	0.03718396231532097
12 time used:  106.24032402038574 lr: [1e-05]
Training_Loss At Epoch 13:	0.03487099793739617
Testing_Loss At Epoch 13:	0.03709987284615636
13 time used:  106.46468567848206 lr: [1e-05]
Training_Loss At Epoch 14:	0.0345711536728777
Testing_Loss At Epoch 14:	0.036745034344494346
14 time used:  106.16801238059998 lr: [1e-05]
15 time used:  105.80660653114319 lr: [1e-05]
Training_Loss At Epoch 16:	0.034128395249135794
Testing_Loss At Epoch 16:	0.036511599123477935
16 time used:  106.13789963722229 lr: [1e-05]
Training_Loss At Epoch 17:	0.033912813938222826
Testing_Loss At Epoch 17:	0.03644065003842115
17 time used:  106.09122896194458 lr: [1e-05]
18 time used:  106.03365278244019 lr: [1e-05]
Training_Loss At Epoch 19:	0.033585969770792874
Testing_Loss At Epoch 19:	0.03625336843542755
19 time used:  106.21176981925964 lr: [1e-05]
Training_Loss At Epoch 20:	0.033469678631518035
Testing_Loss At Epoch 20:	0.03609846659004688
20 time used:  106.25726056098938 lr: [1e-05]
Training_Loss At Epoch 21:	0.033258876237086954
Testing_Loss At Epoch 21:	0.03602968397550285
21 time used:  106.45860934257507 lr: [1e-05]
Training_Loss At Epoch 22:	0.03304315998917445
Testing_Loss At Epoch 22:	0.035991484690457584
22 time used:  106.32574558258057 lr: [1e-05]
Training_Loss At Epoch 23:	0.03294699484249577
Testing_Loss At Epoch 23:	0.035871262066066265
23 time used:  106.34362864494324 lr: [1.0000000000000002e-06]
Training_Loss At Epoch 24:	0.032601777845993635
Testing_Loss At Epoch 24:	0.03560551341623068
24 time used:  106.29449486732483 lr: [1.0000000000000002e-06]
Training_Loss At Epoch 25:	0.032501488353591415
Testing_Loss At Epoch 25:	0.03560284035280347
25 time used:  106.41060972213745 lr: [1.0000000000000002e-06]
Training_Loss At Epoch 26:	0.03249450541334227
Testing_Loss At Epoch 26:	0.035576006248593334
26 time used:  106.52306962013245 lr: [1.0000000000000002e-06]
27 time used:  105.95088958740234 lr: [1.0000000000000002e-06]
28 time used:  106.24417757987976 lr: [1.0000000000000002e-06]
Training_Loss At Epoch 29:	0.032443315098062156
Testing_Loss At Epoch 29:	0.035575842186808586
29 time used:  106.51299381256104 lr: [1.0000000000000002e-06]
Training_Loss At Epoch 30:	0.03239747688174248
Testing_Loss At Epoch 30:	0.0355507292971015
30 time used:  106.39204120635986 lr: [1.0000000000000002e-06]
31 time used:  106.16424870491028 lr: [1.0000000000000002e-06]
Training_Loss At Epoch 32:	0.032345823927316816
Testing_Loss At Epoch 32:	0.03553988505154848
32 time used:  106.30492901802063 lr: [1.0000000000000002e-06]
Training_Loss At Epoch 33:	0.03233785800402984
Testing_Loss At Epoch 33:	0.035501119922846554
33 time used:  106.50618386268616 lr: [1.0000000000000002e-06]
34 time used:  105.91655921936035 lr: [1.0000000000000002e-06]
35 time used:  106.30468535423279 lr: [1.0000000000000002e-07]
36 time used:  106.34153366088867 lr: [1.0000000000000002e-07]
Training_Loss At Epoch 37:	0.03225272473879159
Testing_Loss At Epoch 37:	0.03549752632156014
37 time used:  106.47917199134827 lr: [1.0000000000000002e-07]
38 time used:  106.20457410812378 lr: [1.0000000000000002e-07]
Training_Loss At Epoch 39:	0.032230936284177
Testing_Loss At Epoch 39:	0.03549404033459723
39 time used:  106.60598921775818 lr: [1.0000000000000002e-07]
Training_Loss At Epoch 40:	0.0322598069650121
Testing_Loss At Epoch 40:	0.03548919191583991
40 time used:  106.43940949440002 lr: [1.0000000000000002e-07]
41 time used:  106.14586901664734 lr: [1.0000000000000002e-07]
42 time used:  106.20365333557129 lr: [1.0000000000000002e-07]
43 time used:  106.46633768081665 lr: [1.0000000000000002e-07]
44 time used:  106.11801409721375 lr: [1.0000000000000002e-07]
45 time used:  106.18113374710083 lr: [1.0000000000000002e-07]
Training_Loss At Epoch 46:	0.03222232379252091
Testing_Loss At Epoch 46:	0.03548590714111924
46 time used:  106.67317819595337 lr: [1.0000000000000002e-07]
Training_Loss At Epoch 47:	0.03222030945355073
Testing_Loss At Epoch 47:	0.035472296765074136
47 time used:  106.19075679779053 lr: [1.0000000000000004e-08]
Training_Loss At Epoch 48:	0.03223527522524819
Testing_Loss At Epoch 48:	0.035460464870557186
48 time used:  106.68446779251099 lr: [1.0000000000000004e-08]
49 time used:  106.35562801361084 lr: [1.0000000000000004e-08]
50 time used:  106.33125734329224 lr: [1.0000000000000004e-08]
51 time used:  106.09221410751343 lr: [1.0000000000000004e-08]
52 time used:  106.17206907272339 lr: [1.0000000000000004e-08]
